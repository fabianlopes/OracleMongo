import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain import hub
from agents.tools import buscar_demandas_por_status, estatisticas_por_realizador, analisar_linha_tempo_ticket, identificar_demandas_duplicadas

load_dotenv()

def iniciar_orquestrador_gestao():
    # Usando GPT-4 pela precis√£o em an√°lise de regras de neg√≥cio
    #llm = ChatOpenAI(model="gpt-4-turbo", temperature=0)
    llm = ChatOllama(model="llama3.1", temperature=0)

    # Lista de ferramentas que o gestor ter√° √† disposi√ß√£o
    tools = [
        buscar_demandas_por_status,
        estatisticas_por_realizador,
        analisar_linha_tempo_ticket,
        identificar_demandas_duplicadas
    ]

    # Prompt de Sistema focado em Gerenciamento de Projetos e Pipeline
    prompt_sistema = """Voc√™ √© o Orquestrador de Projetos da SEMEF para o sistema CITSM.
    Sua prioridade absoluta √© o gerenciamento de 'Demandas em Andamento'.
    
    Suas diretrizes de gest√£o s√£o:
    1. **Prioridade M√°xima**: Monitorar demandas em andamento e garantir que os 'Principais Realizadores' n√£o estejam sobrecarregados.
    2. **Monitor de Bloqueios**: Identificar demandas 'Suspensas por falta de informa√ß√£o' ou 'Suspensas por Homologa√ß√£o/Testes'.
    3. **Qualidade**: Detectar demandas duplicadas comparando resumos e descri√ß√µes.
    4. **Pipeline**: Analisar a linha do tempo para identificar tickets que n√£o sofrem altera√ß√£o h√° muito tempo.
    5. **Identificar e LISTAR duplicados existentes para evitar retrabalho. Ao identificar duplicatas, apresente-as em formato de lista, mostrando o 
    'Ticket Principal' e o 'Resumo' comum entre eles.
    
    Sempre que encontrar um problema (ex: t√©cnico sobrecarregado ou ticket parado em homologa√ß√£o), sugira uma a√ß√£o corretiva para o Gestor."""

    # Puxando o template de prompt padr√£o para agentes de fun√ß√£o
    base_prompt = hub.pull("hwchase17/openai-functions-agent")

    # Adicionando o nosso contexto ao prompt base
    full_prompt = base_prompt.partial(instructions=prompt_sistema)

    # Criando o agente
    agent = create_openai_functions_agent(llm, tools, full_prompt)

    return AgentExecutor(agent=agent, tools=tools, verbose=True)

if __name__ == "__main__":
    pmo_virtual = iniciar_orquestrador_gestao()

    # Exemplo de consulta de gest√£o
    print("ü§ñ Iniciando consulta de gerenciamento...")
    pmo_virtual.invoke({
        "input": "Como est√° a carga de trabalho atual? Existem demandas em andamento que est√£o paradas h√° muito tempo em homologa√ß√£o?"
    })